[dataset]
# tensor type (float or double)
tensor_type = float

task_type = seg 
train_dir = ../../PyMIC_data/Promise12/preprocess
train_csv = config/data/image_train.csv
valid_csv = config/data/image_valid.csv
test_csv  = config/data/image_test.csv

train_batch_size = 4
patch_size = [64, 128, 128]
# data transforms
train_transform = [Pad, RandomCrop, RandomFlip, NormalizeWithMeanStd, GammaCorrection, GaussianNoise, LabelToProbability]
valid_transform = [NormalizeWithMeanStd, Pad, LabelToProbability]
test_transform  = [NormalizeWithMeanStd, Pad]

RandomFlip_flip_depth  = True
RandomFlip_flip_height = True
RandomFlip_flip_width  = True

NormalizeWithMeanStd_channels = [0]

GammaCorrection_channels  = [0]
GammaCorrection_gamma_min = 0.7
GammaCorrection_gamma_max = 1.5

GaussianNoise_channels = [0]
GaussianNoise_mean     = 0
GaussianNoise_std      = 0.05
GaussianNoise_probability = 0.5

[network]
# type of network
net_type = UNet2D5

# number of class, required for segmentation task
class_num = 2
in_chns       = 1
feature_chns  = [32, 64, 128, 256, 512]
dropout       = [0, 0, 0.2, 0.2, 0.2]
conv_dims     = [2, 2, 3, 3, 3]
multiscale_pred = True

[training]
# list of gpus
gpus       = [0]

loss_type     = [DiceLoss, CrossEntropyLoss]
loss_weight   = [1.0, 1.0]
deep_supervise = True

# for optimizers
optimizer     = Adam
learning_rate = 1e-3
momentum      = 0.9
weight_decay  = 1e-5

# for lr scheduler (StepLR)
lr_scheduler = PolynomialLR
lr_power     = 0.8
early_stop_patience = 1000
ckpt_dir    = model/unet2d5

# start iter
iter_max   = 4000
iter_valid = 250
iter_save  = 4000


[testing]
# list of gpus
gpus       = [0]

# checkpoint mode can be [0-latest, 1-best, 2-specified]
ckpt_mode         = 1
output_dir        = result/unet2d5
post_process      = KeepLargestComponent

sliding_window_enable = True
sliding_window_size   = [64, 128, 128]
sliding_window_stride = [64, 128, 128]

