[dataset]
# tensor type (float or double)
tensor_type    = float
task_type      = rec
supervise_type = self_sup

# Set this value according to the path of dataset
train_dir = /home/disk4t/data/lung/LUNA2016/preprocess
valid_dir = ./pretrain_valid/genesis
test_dir  = ./pretrain_valid/genesis
train_csv = config/luna_data/luna_train.csv
valid_csv = config/luna_data/luna_valid.csv
test_csv  = config/luna_data/luna_valid.csv

train_batch_size = 2
num_worker       = 4
patch_size       = [64, 128, 128]

train_transform = [RandomCrop, SelfReconstructionLabel, LocalShuffling, NonLinearTransform, InOutPainting]
valid_transform = None 
test_transform  = None 
RandomCrop_output_size  = [64, 128, 128]

LocalShuffling_probability     = 1.0
LocalShuffling_block_range     = [32, 64]
LocalShuffling_block_size      = [8, 16, 16]
NonLinearTransform_probability = 0.8
NonLinearTransform_block_range = [32, 64]
NonLinearTransform_block_size  = [8, 16, 16]
InOutPainting_probability      = 0.9
InPainting_probability         = 0.8
InPainting_block_range         = [32, 64]
InPainting_block_size          = [8, 16, 16]

[network]
# type of network
net_type = UNet3D

# number of class, required for segmentation task
class_num     = 1
in_chns       = 1
feature_chns  = [32, 64, 128, 256, 512]
dropout       = [0, 0, 0.2, 0.2, 0.2]
up_mode       = 2
weights_load_mode = encoder
multiscale_pred = False

[self_supervised_learning]
method_name = ModelGenesis

[training]
# list of gpus
gpus       = [0]

loss_type     = MAELoss
loss_acti_func  = tanh

# for optimizers
optimizer     = Adam
learning_rate = 1e-3
momentum      = 0.9
weight_decay  = 1e-5

# for lr schedular
lr_scheduler = StepLR
lr_gamma = 0.5
lr_step  = 20000
early_stop_patience = 20000
ckpt_dir       = pretrain_model/unet3d_genesis

iter_max   = 80000
iter_valid = 1000
iter_save  = 40000

[testing]
# list of gpus
gpus       = [0]
evaluation_mode   = True
sliding_window_enable = False