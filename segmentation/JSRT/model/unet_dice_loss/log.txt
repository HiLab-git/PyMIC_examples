dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 0
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
2022-08-17 16:55:14 training start

2022-08-17 16:55:25 it 200
learning rate 0.001
training/validation time: 10.21s/0.72s
train loss 0.1217, avg dice 0.9504 [0.9639 0.9369]
valid loss 0.0447, avg dice 0.9742 [0.9848 0.9636]

2022-08-17 16:55:36 it 400
learning rate 0.001
training/validation time: 10.42s/0.77s
train loss 0.0300, avg dice 0.9797 [0.9859 0.9735]
valid loss 0.0246, avg dice 0.9813 [0.9890 0.9735]

2022-08-17 16:55:47 it 600
learning rate 0.001
training/validation time: 10.73s/0.70s
train loss 0.0207, avg dice 0.9830 [0.9882 0.9778]
valid loss 0.0202, avg dice 0.9825 [0.9897 0.9753]

2022-08-17 16:55:58 it 800
learning rate 0.001
training/validation time: 9.98s/0.75s
train loss 0.0186, avg dice 0.9834 [0.9884 0.9784]
valid loss 0.0193, avg dice 0.9825 [0.9896 0.9753]

2022-08-17 16:56:09 it 1000
learning rate 0.001
training/validation time: 10.67s/0.74s
train loss 0.0162, avg dice 0.9851 [0.9896 0.9805]
valid loss 0.0171, avg dice 0.9840 [0.9907 0.9773]

2022-08-17 16:56:20 it 1200
learning rate 0.001
training/validation time: 9.99s/0.75s
train loss 0.0149, avg dice 0.9860 [0.9903 0.9818]
valid loss 0.0170, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 16:56:32 it 1400
learning rate 0.001
training/validation time: 10.57s/0.74s
train loss 0.0166, avg dice 0.9841 [0.9890 0.9792]
valid loss 0.0179, avg dice 0.9827 [0.9899 0.9756]

2022-08-17 16:56:42 it 1600
learning rate 0.001
training/validation time: 10.03s/0.74s
train loss 0.0145, avg dice 0.9861 [0.9903 0.9818]
valid loss 0.0164, avg dice 0.9841 [0.9907 0.9775]

2022-08-17 16:56:53 it 1800
learning rate 0.001
training/validation time: 10.14s/0.75s
train loss 0.0149, avg dice 0.9855 [0.9900 0.9810]
valid loss 0.0166, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 16:57:04 it 2000
learning rate 0.001
training/validation time: 10.48s/0.76s
train loss 0.0138, avg dice 0.9865 [0.9906 0.9824]
valid loss 0.0166, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 16:57:15 it 2200
learning rate 0.0005
training/validation time: 9.93s/0.80s
train loss 0.0125, avg dice 0.9878 [0.9916 0.9841]
valid loss 0.0164, avg dice 0.9839 [0.9906 0.9772]

2022-08-17 16:57:27 it 2400
learning rate 0.0005
training/validation time: 10.61s/0.75s
train loss 0.0122, avg dice 0.9881 [0.9917 0.9845]
valid loss 0.0161, avg dice 0.9842 [0.9907 0.9776]

2022-08-17 16:57:37 it 2600
learning rate 0.0005
training/validation time: 9.99s/0.73s
train loss 0.0121, avg dice 0.9882 [0.9918 0.9845]
valid loss 0.0162, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 16:57:49 it 2800
learning rate 0.0005
training/validation time: 10.58s/0.74s
train loss 0.0122, avg dice 0.9881 [0.9917 0.9844]
valid loss 0.0162, avg dice 0.9840 [0.9906 0.9773]

2022-08-17 16:57:59 it 3000
learning rate 0.0005
training/validation time: 10.05s/0.74s
train loss 0.0120, avg dice 0.9883 [0.9918 0.9847]
valid loss 0.0173, avg dice 0.9829 [0.9899 0.9760]

2022-08-17 16:58:11 it 3200
learning rate 0.0005
training/validation time: 10.67s/0.73s
train loss 0.0118, avg dice 0.9884 [0.9920 0.9849]
valid loss 0.0164, avg dice 0.9838 [0.9904 0.9772]

2022-08-17 16:58:22 it 3400
learning rate 0.0005
training/validation time: 10.22s/0.74s
train loss 0.0118, avg dice 0.9884 [0.9919 0.9848]
valid loss 0.0168, avg dice 0.9833 [0.9902 0.9765]

2022-08-17 16:58:33 it 3600
learning rate 0.0005
training/validation time: 10.22s/0.76s
train loss 0.0127, avg dice 0.9875 [0.9913 0.9837]
valid loss 0.0167, avg dice 0.9835 [0.9902 0.9767]

2022-08-17 16:58:44 it 3800
learning rate 0.0005
training/validation time: 10.63s/0.75s
train loss 0.0115, avg dice 0.9886 [0.9921 0.9851]
valid loss 0.0161, avg dice 0.9841 [0.9906 0.9775]

2022-08-17 16:58:55 it 4000
learning rate 0.0005
training/validation time: 10.04s/0.79s
train loss 0.0111, avg dice 0.9890 [0.9924 0.9857]
valid loss 0.0169, avg dice 0.9832 [0.9902 0.9763]

2022-08-17 16:59:06 it 4200
learning rate 0.00025
training/validation time: 10.62s/0.76s
train loss 0.0105, avg dice 0.9896 [0.9928 0.9865]
valid loss 0.0159, avg dice 0.9842 [0.9907 0.9776]

2022-08-17 16:59:17 it 4400
learning rate 0.00025
training/validation time: 10.05s/0.76s
train loss 0.0104, avg dice 0.9897 [0.9929 0.9866]
valid loss 0.0168, avg dice 0.9833 [0.9902 0.9764]

2022-08-17 16:59:29 it 4600
learning rate 0.00025
training/validation time: 10.63s/0.66s
train loss 0.0105, avg dice 0.9897 [0.9928 0.9865]
valid loss 0.0159, avg dice 0.9843 [0.9907 0.9778]

2022-08-17 16:59:39 it 4800
learning rate 0.00025
training/validation time: 9.97s/0.75s
train loss 0.0103, avg dice 0.9899 [0.9929 0.9868]
valid loss 0.0164, avg dice 0.9837 [0.9904 0.9770]

2022-08-17 16:59:51 it 5000
learning rate 0.00025
training/validation time: 10.65s/0.76s
train loss 0.0101, avg dice 0.9900 [0.9931 0.9869]
valid loss 0.0169, avg dice 0.9832 [0.9900 0.9763]

2022-08-17 17:00:02 it 5200
learning rate 0.00025
training/validation time: 10.35s/0.76s
train loss 0.0100, avg dice 0.9901 [0.9931 0.9871]
valid loss 0.0163, avg dice 0.9838 [0.9905 0.9771]

2022-08-17 17:00:13 it 5400
learning rate 0.00025
training/validation time: 10.11s/0.78s
train loss 0.0105, avg dice 0.9896 [0.9927 0.9864]
valid loss 0.0161, avg dice 0.9840 [0.9906 0.9773]

2022-08-17 17:00:25 it 5600
learning rate 0.00025
training/validation time: 11.21s/0.76s
train loss 0.0100, avg dice 0.9901 [0.9931 0.9871]
valid loss 0.0161, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 17:00:36 it 5800
learning rate 0.00025
training/validation time: 10.31s/0.73s
train loss 0.0098, avg dice 0.9903 [0.9932 0.9874]
valid loss 0.0159, avg dice 0.9842 [0.9907 0.9777]

2022-08-17 17:00:47 it 6000
learning rate 0.00025
training/validation time: 10.97s/0.75s
train loss 0.0098, avg dice 0.9903 [0.9933 0.9874]
valid loss 0.0158, avg dice 0.9842 [0.9907 0.9777]

2022-08-17 17:00:58 it 6200
learning rate 0.000125
training/validation time: 10.23s/0.76s
train loss 0.0094, avg dice 0.9907 [0.9936 0.9879]
valid loss 0.0157, avg dice 0.9843 [0.9908 0.9779]

2022-08-17 17:01:10 it 6400
learning rate 0.000125
training/validation time: 10.93s/0.74s
train loss 0.0092, avg dice 0.9909 [0.9937 0.9882]
valid loss 0.0160, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 17:01:21 it 6600
learning rate 0.000125
training/validation time: 10.35s/0.75s
train loss 0.0091, avg dice 0.9910 [0.9937 0.9882]
valid loss 0.0162, avg dice 0.9838 [0.9905 0.9772]

2022-08-17 17:01:33 it 6800
learning rate 0.000125
training/validation time: 10.97s/0.74s
train loss 0.0090, avg dice 0.9911 [0.9938 0.9883]
valid loss 0.0162, avg dice 0.9839 [0.9906 0.9773]

2022-08-17 17:01:44 it 7000
learning rate 0.000125
training/validation time: 10.16s/0.75s
train loss 0.0090, avg dice 0.9910 [0.9938 0.9883]
valid loss 0.0160, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 17:01:55 it 7200
learning rate 0.000125
training/validation time: 10.31s/0.75s
train loss 0.0089, avg dice 0.9911 [0.9938 0.9885]
valid loss 0.0165, avg dice 0.9835 [0.9904 0.9767]

2022-08-17 17:02:06 it 7400
learning rate 0.000125
training/validation time: 10.84s/0.74s
train loss 0.0089, avg dice 0.9912 [0.9939 0.9885]
valid loss 0.0164, avg dice 0.9836 [0.9903 0.9769]

2022-08-17 17:02:17 it 7600
learning rate 0.000125
training/validation time: 10.21s/0.75s
train loss 0.0088, avg dice 0.9913 [0.9939 0.9887]
valid loss 0.0165, avg dice 0.9836 [0.9904 0.9768]

2022-08-17 17:02:29 it 7800
learning rate 0.000125
training/validation time: 10.91s/0.75s
train loss 0.0088, avg dice 0.9913 [0.9940 0.9887]
valid loss 0.0164, avg dice 0.9837 [0.9904 0.9769]

2022-08-17 17:02:40 it 8000
learning rate 0.000125
training/validation time: 10.18s/0.75s
train loss 0.0087, avg dice 0.9914 [0.9940 0.9888]
valid loss 0.0165, avg dice 0.9836 [0.9904 0.9767]
The best performing iter is 6200, valid dice 0.9843176603317261
dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 0
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
testing time 0.007723640888295275 +/- 0.02240574199733605
dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 1
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
testing time 0.007773145716241065 +/- 0.021858428671199033
dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 0
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
testing time 0.007784097752672561 +/- 0.02273100533601668
dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 0
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
2022-08-17 20:16:25 training start

2022-08-17 20:16:37 it 200
learning rate 0.001
training/validation time: 10.53s/0.77s
train loss 0.1217, avg dice 0.9504 [0.9639 0.9369]
valid loss 0.0447, avg dice 0.9742 [0.9848 0.9636]

2022-08-17 20:16:48 it 400
learning rate 0.001
training/validation time: 10.17s/0.74s
train loss 0.0300, avg dice 0.9797 [0.9859 0.9735]
valid loss 0.0246, avg dice 0.9813 [0.9890 0.9735]

2022-08-17 20:16:59 it 600
learning rate 0.001
training/validation time: 10.70s/0.75s
train loss 0.0207, avg dice 0.9830 [0.9882 0.9778]
valid loss 0.0202, avg dice 0.9825 [0.9897 0.9753]

2022-08-17 20:17:10 it 800
learning rate 0.001
training/validation time: 10.02s/0.76s
train loss 0.0186, avg dice 0.9834 [0.9884 0.9784]
valid loss 0.0193, avg dice 0.9825 [0.9896 0.9753]

2022-08-17 20:17:21 it 1000
learning rate 0.001
training/validation time: 10.78s/0.75s
train loss 0.0162, avg dice 0.9851 [0.9896 0.9805]
valid loss 0.0171, avg dice 0.9840 [0.9907 0.9773]

2022-08-17 20:17:32 it 1200
learning rate 0.001
training/validation time: 10.00s/0.71s
train loss 0.0149, avg dice 0.9860 [0.9903 0.9818]
valid loss 0.0170, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 20:17:44 it 1400
learning rate 0.001
training/validation time: 10.74s/0.77s
train loss 0.0166, avg dice 0.9841 [0.9890 0.9792]
valid loss 0.0179, avg dice 0.9827 [0.9899 0.9756]

2022-08-17 20:17:55 it 1600
learning rate 0.001
training/validation time: 10.22s/0.75s
train loss 0.0145, avg dice 0.9861 [0.9903 0.9818]
valid loss 0.0164, avg dice 0.9841 [0.9907 0.9775]

2022-08-17 20:18:05 it 1800
learning rate 0.001
training/validation time: 9.84s/0.77s
train loss 0.0149, avg dice 0.9855 [0.9900 0.9810]
valid loss 0.0166, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 20:18:17 it 2000
learning rate 0.001
training/validation time: 10.64s/0.75s
train loss 0.0138, avg dice 0.9865 [0.9906 0.9824]
valid loss 0.0166, avg dice 0.9837 [0.9905 0.9770]

2022-08-17 20:18:27 it 2200
learning rate 0.0005
training/validation time: 10.17s/0.75s
train loss 0.0125, avg dice 0.9878 [0.9916 0.9841]
valid loss 0.0164, avg dice 0.9839 [0.9906 0.9772]

2022-08-17 20:18:39 it 2400
learning rate 0.0005
training/validation time: 10.93s/0.74s
train loss 0.0122, avg dice 0.9881 [0.9917 0.9845]
valid loss 0.0161, avg dice 0.9842 [0.9907 0.9776]

2022-08-17 20:18:50 it 2600
learning rate 0.0005
training/validation time: 10.24s/0.73s
train loss 0.0121, avg dice 0.9882 [0.9918 0.9845]
valid loss 0.0162, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 20:19:02 it 2800
learning rate 0.0005
training/validation time: 10.79s/0.75s
train loss 0.0122, avg dice 0.9881 [0.9917 0.9844]
valid loss 0.0162, avg dice 0.9840 [0.9906 0.9773]

2022-08-17 20:19:13 it 3000
learning rate 0.0005
training/validation time: 10.15s/0.75s
train loss 0.0120, avg dice 0.9883 [0.9918 0.9847]
valid loss 0.0173, avg dice 0.9829 [0.9899 0.9760]

2022-08-17 20:19:24 it 3200
learning rate 0.0005
training/validation time: 10.98s/0.75s
train loss 0.0118, avg dice 0.9884 [0.9920 0.9849]
valid loss 0.0164, avg dice 0.9838 [0.9904 0.9772]

2022-08-17 20:19:35 it 3400
learning rate 0.0005
training/validation time: 10.19s/0.69s
train loss 0.0118, avg dice 0.9884 [0.9919 0.9848]
valid loss 0.0168, avg dice 0.9833 [0.9902 0.9765]

2022-08-17 20:19:46 it 3600
learning rate 0.0005
training/validation time: 9.92s/0.80s
train loss 0.0127, avg dice 0.9875 [0.9913 0.9837]
valid loss 0.0167, avg dice 0.9835 [0.9902 0.9767]

2022-08-17 20:19:57 it 3800
learning rate 0.0005
training/validation time: 10.78s/0.74s
train loss 0.0115, avg dice 0.9886 [0.9921 0.9851]
valid loss 0.0161, avg dice 0.9841 [0.9906 0.9775]

2022-08-17 20:20:08 it 4000
learning rate 0.0005
training/validation time: 10.08s/0.79s
train loss 0.0111, avg dice 0.9890 [0.9924 0.9857]
valid loss 0.0169, avg dice 0.9832 [0.9902 0.9763]

2022-08-17 20:20:20 it 4200
learning rate 0.00025
training/validation time: 10.78s/0.76s
train loss 0.0105, avg dice 0.9896 [0.9928 0.9865]
valid loss 0.0159, avg dice 0.9842 [0.9907 0.9776]

2022-08-17 20:20:31 it 4400
learning rate 0.00025
training/validation time: 10.15s/0.79s
train loss 0.0104, avg dice 0.9897 [0.9929 0.9866]
valid loss 0.0168, avg dice 0.9833 [0.9902 0.9764]

2022-08-17 20:20:42 it 4600
learning rate 0.00025
training/validation time: 10.71s/0.75s
train loss 0.0105, avg dice 0.9897 [0.9928 0.9865]
valid loss 0.0159, avg dice 0.9843 [0.9907 0.9778]

2022-08-17 20:20:53 it 4800
learning rate 0.00025
training/validation time: 10.29s/0.75s
train loss 0.0103, avg dice 0.9899 [0.9929 0.9868]
valid loss 0.0164, avg dice 0.9837 [0.9904 0.9770]

2022-08-17 20:21:05 it 5000
learning rate 0.00025
training/validation time: 10.72s/0.74s
train loss 0.0101, avg dice 0.9900 [0.9931 0.9869]
valid loss 0.0169, avg dice 0.9832 [0.9900 0.9763]

2022-08-17 20:21:16 it 5200
learning rate 0.00025
training/validation time: 10.41s/0.76s
train loss 0.0100, avg dice 0.9901 [0.9931 0.9871]
valid loss 0.0163, avg dice 0.9838 [0.9905 0.9771]

2022-08-17 20:21:27 it 5400
learning rate 0.00025
training/validation time: 10.37s/0.78s
train loss 0.0105, avg dice 0.9896 [0.9927 0.9864]
valid loss 0.0161, avg dice 0.9840 [0.9906 0.9773]

2022-08-17 20:21:39 it 5600
learning rate 0.00025
training/validation time: 11.04s/0.76s
train loss 0.0100, avg dice 0.9901 [0.9931 0.9871]
valid loss 0.0161, avg dice 0.9840 [0.9906 0.9774]

2022-08-17 20:21:50 it 5800
learning rate 0.00025
training/validation time: 10.45s/0.75s
train loss 0.0098, avg dice 0.9903 [0.9932 0.9874]
valid loss 0.0159, avg dice 0.9842 [0.9907 0.9777]

2022-08-17 20:22:02 it 6000
learning rate 0.00025
training/validation time: 11.23s/0.74s
train loss 0.0098, avg dice 0.9903 [0.9933 0.9874]
valid loss 0.0158, avg dice 0.9842 [0.9907 0.9777]

2022-08-17 20:22:13 it 6200
learning rate 0.000125
training/validation time: 10.21s/0.79s
train loss 0.0094, avg dice 0.9907 [0.9936 0.9879]
valid loss 0.0157, avg dice 0.9843 [0.9908 0.9779]
dataset tensor_type = float
dataset task_type = seg
dataset root_dir = /home/disk2t/projects/PyMIC_project/PyMIC_data/JSRT
dataset train_csv = config/jsrt_train.csv
dataset valid_csv = config/jsrt_valid.csv
dataset test_csv = config/jsrt_test.csv
dataset train_batch_size = 4
dataset train_transform = ['NormalizeWithMeanStd', 'RandomCrop', 'LabelConvert', 'LabelToProbability']
dataset valid_transform = ['NormalizeWithMeanStd', 'LabelConvert', 'LabelToProbability']
dataset test_transform = ['NormalizeWithMeanStd']
dataset normalizewithmeanstd_channels = [0]
dataset randomcrop_output_size = [240, 240]
dataset labelconvert_source_list = [0, 255]
dataset labelconvert_target_list = [0, 1]
dataset labeltoprobability_class_num = 2
network net_type = UNet2D
network class_num = 2
network in_chns = 1
network feature_chns = [16, 32, 64, 128, 256]
network dropout = [0, 0, 0.3, 0.4, 0.5]
network bilinear = False
network deep_supervise = False
training gpus = [0]
training loss_type = DiceLoss
training optimizer = Adam
training learning_rate = 0.001
training momentum = 0.9
training weight_decay = 1e-05
training lr_scheduler = MultiStepLR
training lr_gamma = 0.5
training lr_milestones = [2000, 4000, 6000]
training ckpt_save_dir = model/unet_dice_loss
training ckpt_prefix = unet
training iter_start = 0
training iter_max = 8000
training iter_valid = 200
training iter_save = 8000
testing gpus = [0]
testing ckpt_mode = 0
testing output_dir = result
testing label_source = [0, 1]
testing label_target = [0, 255]
deterministric is true
parameter number 1943778
testing time 0.007734126233040018 +/- 0.022255747441987103
